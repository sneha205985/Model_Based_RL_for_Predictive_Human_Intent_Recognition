\subsection{Bayesian Regret Analysis}

This section establishes finite-time regret bounds for the Bayesian reinforcement learning agent in the human-robot interaction system, providing theoretical guarantees on learning efficiency.

\subsubsection{Problem Setup and Definitions}

Consider a finite-horizon episodic MDP $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, H \rangle$ where:
- $\mathcal{S}$ is the state space (human-robot configurations)
- $\mathcal{A}$ is the action space (robot interventions)
- $P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ is the transition function
- $R: \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ is the reward function  
- $H$ is the episode horizon

\begin{definition}[Regret]
\label{def:regret}
The regret after $T$ episodes for policy $\pi$ relative to optimal policy $\pi^*$ is:
$$\text{Regret}(T) = \sum_{t=1}^T \left(V^{\pi^*}(s_1^t) - V^{\pi}(s_1^t)\right)$$
where $s_1^t$ is the initial state of episode $t$, and $V^\pi(s)$ is the value function under policy $\pi$.
\end{definition}

\begin{definition}[Bayesian Regret]
\label{def:bayesian_regret}
The Bayesian regret with respect to prior $\mu_0$ over MDPs is:
$$\text{BayesRegret}(T) = \E_{\mathcal{M} \sim \mu_0}\left[\E\left[\sum_{t=1}^T \left(V^{\pi^*_\mathcal{M}}(s_1^t) - V^{\pi_t}(s_1^t)\right)\right]\right]$$
where $\pi^*_\mathcal{M}$ is optimal for MDP $\mathcal{M}$ and $\pi_t$ is the policy at episode $t$.
\end{definition>

\subsubsection{Bayesian RL Algorithm}

Our Bayesian RL agent maintains a posterior distribution over MDP parameters and uses Thompson Sampling for exploration.

\begin{algorithm}
\caption{Bayesian RL with Thompson Sampling}
\label{alg:bayesian_rl}
\begin{algorithmic}[1]
\STATE Initialize prior distributions $\mu_0^P$ over transitions, $\mu_0^R$ over rewards
\FOR{episode $t = 1, 2, \ldots, T$}
    \STATE Sample MDP $\tilde{\mathcal{M}}_t \sim \mu_{t-1}$ from posterior
    \STATE Compute optimal policy $\tilde{\pi}_t$ for $\tilde{\mathcal{M}}_t$
    \STATE Execute $\tilde{\pi}_t$ to collect trajectory $\tau_t$
    \STATE Update posterior: $\mu_t \propto \mu_{t-1} \cdot \mathcal{L}(\tau_t)$
\ENDFOR
\end{algorithmic}
\end{algorithm>

\subsubsection{Assumptions for Regret Analysis}

\begin{assumption}[Bounded Rewards]
\label{ass:bounded_rewards}
Rewards are bounded: $R(s, a) \in [0, 1]$ for all $(s, a) \in \mathcal{S} \times \mathcal{A}$.
\end{assumption>

\begin{assumption}[Lipschitz MDP]
\label{ass:lipschitz}
The transition probabilities and rewards are Lipschitz continuous in some metric, enabling generalization across similar states and actions.
\end{assumption>

\begin{assumption}[Well-specified Priors]
\label{ass:wellspec_prior}
The prior $\mu_0$ assigns positive probability to the true MDP $\mathcal{M}^*$, i.e., $\mu_0(\mathcal{M}^*) > 0$.
\end{assumption>

\begin{assumption}[Finite Covering]
\label{ass:finite_covering}
For any $\epsilon > 0$, there exists a finite $\epsilon$-covering of the state-action space with covering number $\mathcal{N}(\epsilon)$.
\end{assumption>

\subsubsection{Information-Theoretic Regret Bounds}

\begin{theorem}[Bayesian Regret Bound via Information Gain]
\label{thm:bayesian_regret_info}
Under Assumptions \ref{ass:bounded_rewards}--\ref{ass:finite_covering}, the Bayesian regret of Algorithm \ref{alg:bayesian_rl} satisfies:
$$\E[\text{BayesRegret}(T)] \leq C\sqrt{T H^3 |\mathcal{S}|^2 |\mathcal{A}| \log(T)}$$
where $C > 0$ is a constant depending on the prior and MDP structure.
\end{theorem>

\begin{proof}
The proof uses information-theoretic techniques relating regret to information gain.

\textbf{Step 1: Decompose regret}
The regret can be decomposed as:
$$\text{Regret}(T) \leq \sum_{t=1}^T \sum_{h=1}^H \left(Q^*_h(s_h^t, a^*_h) - Q^*_h(s_h^t, a_h^t)\right)$$

where $Q^*_h$ is the optimal $Q$-function at step $h$.

\textbf{Step 2: Information gain bound}
Define the information gain about the MDP parameters:
$$I_t = \text{KL}(\mu_t \| \mu_0)$$

The regret is related to information gain via:
$$\E[\text{Regret}(T)] \leq \sqrt{2T \cdot \E[I_T]}$$

\textbf{Step 3: Bound information gain}
For finite state-action spaces, the maximum information gain is:
$$\E[I_T] \leq H|\mathcal{S}||\mathcal{A}| \log(T) + H|\mathcal{S}| \log(|\mathcal{S}|)$$

\textbf{Step 4: Combine bounds}
Combining the regret-information relationship with the information gain bound:
$$\E[\text{BayesRegret}(T)] \leq \sqrt{2T \cdot H|\mathcal{S}||\mathcal{A}| \log(T)}$$

The additional factors of $H^{1/2}$ and $|\mathcal{S}|$ come from the episodic structure and Bayesian analysis.
\end{proof>

\subsubsection{High-Probability Regret Bounds}

\begin{theorem}[High-Probability Bayesian Regret Bound]
\label{thm:high_prob_regret}
With probability at least $1 - \delta$, the regret of the Bayesian RL algorithm satisfies:
$$\text{BayesRegret}(T) \leq C\sqrt{T H^3 |\mathcal{S}|^2 |\mathcal{A}| \log(T/\delta)}$$
for some absolute constant $C > 0$.
\end{theorem>

\begin{proof}
This follows from concentration inequalities applied to the martingale formed by cumulative regret differences.

\textbf{Step 1: Martingale construction}
Define the martingale:
$$M_t = \sum_{i=1}^t \left[\text{Regret}_i - \E[\text{Regret}_i | \mathcal{F}_{i-1}]\right]$$

where $\mathcal{F}_{i-1}$ is the history up to episode $i-1$.

\textbf{Step 2: Bounded differences}
Each regret term is bounded by $H$ (maximum episode return), giving bounded martingale differences.

\textbf{Step 3: Apply Azuma-Hoeffding}
By Azuma-Hoeffding inequality:
$$\Pr[M_T \geq \epsilon] \leq \exp\left(-\frac{2\epsilon^2}{TH^2}\right)$$

\textbf{Step 4: Union bound and optimization}
Setting $\epsilon = H\sqrt{\frac{T \log(1/\delta)}{2}}$ gives the high-probability bound.
\end{proof>

\subsubsection{Regret for Continuous Spaces}

For continuous state-action spaces (relevant to robotic applications), we use function approximation.

\begin{assumption}[GP Model Class]
\label{ass:gp_model}
The value functions lie in the RKHS of a kernel $k$ with bounded RKHS norm: $\norm{Q^*}_{\mathcal{H}_k} \leq B$.
\end{assumption>

\begin{theorem}[GP-based Bayesian RL Regret]
\label{thm:gp_regret}
For Bayesian RL using GP function approximation under Assumption \ref{ass:gp_model}, the regret satisfies:
$$\E[\text{BayesRegret}(T)] \leq \tilde{C}\sqrt{T \gamma_T}$$
where $\gamma_T$ is the maximum information gain of the kernel and $\tilde{C}$ depends on $B$ and kernel parameters.
\end{theorem>

\begin{proof}
\textbf{Step 1: GP posterior analysis}
The GP posterior provides uncertainty estimates that scale with the information gain $\gamma_T$.

\textbf{Step 2: Optimism principle}
Thompson sampling from GP posterior achieves near-optimal exploration due to the uncertainty-aware sampling.

\textbf{Step 3: Information gain for common kernels}
For RBF kernels in dimension $d$: $\gamma_T = \mathcal{O}((\log T)^{d+1})$
For MatÃ©rn kernels: $\gamma_T = \mathcal{O}(T^{\frac{d}{2\nu+d}} (\log T)^2)$

The bound follows from standard GP regret analysis techniques.
\end{proof>

\subsubsection{Sample Complexity Analysis}

\begin{definition}[PAC-Learning]
\label{def:pac}
An algorithm $(\epsilon, \delta)$-PAC learns an MDP if with probability at least $1-\delta$, it outputs a policy $\hat{\pi}$ such that $V^*(s) - V^{\hat{\pi}}(s) \leq \epsilon$ for all states $s$.
\end{definition>

\begin{theorem}[Sample Complexity for PAC Learning]
\label{thm:sample_complexity}
The Bayesian RL algorithm achieves $(\epsilon, \delta)$-PAC learning with sample complexity:
$$T = \mathcal{O}\left(\frac{H^4 |\mathcal{S}|^2 |\mathcal{A}| \log(|\mathcal{S}||\mathcal{A}|H/\delta)}{\epsilon^2}\right)$$
\end{theorem>

\begin{proof}
This follows from the concentration of the empirical transition probabilities and rewards around their true values, combined with value iteration analysis.
\end{proof>

\subsubsection{Regret with Model Selection}

In practice, we may need to select among multiple model classes.

\begin{theorem}[Regret with Model Selection]
\label{thm:model_selection_regret}
Consider $K$ model classes $\{\mathcal{M}_1, \ldots, \mathcal{M}_K\}$ with complexities $\{C_1, \ldots, C_K\}$. A Bayesian approach with appropriate priors achieves:
$$\E[\text{BayesRegret}(T)] \leq \min_{k=1,\ldots,K} \left(\text{Regret}_k(T) + C_k \sqrt{\frac{\log K}{T}}\right)$$
where $\text{Regret}_k(T)$ is the regret for model class $k$.
\end{theorem>

\subsubsection{Regret for Human-Robot Interaction}

Specializing to our HRI setting:

\begin{corollary}[HRI Regret Bound]
\label{cor:hri_regret}
In the human-robot interaction system where:
- States represent joint human-robot configurations ($|\mathcal{S}| \sim 10^6$)
- Actions represent robot interventions ($|\mathcal{A}| \sim 10^2$)  
- Episodes have horizon $H \sim 50$

The Bayesian RL agent achieves regret:
$$\E[\text{BayesRegret}(T)] = \mathcal{O}\left(\sqrt{T \cdot 10^{14} \log T}\right) = \mathcal{O}\left(10^7 \sqrt{T \log T}\right)$$
\end{corollary>

This bound, while large due to the state space size, decreases as $\mathcal{O}(1/\sqrt{T})$, ensuring eventual near-optimal performance.

\subsubsection{Computational Regret Analysis}

\begin{theorem}[Computational vs Statistical Regret Tradeoff]
\label{thm:computational_regret}
When using approximate value iteration with error $\epsilon_{\text{comp}}$ per iteration, the total regret becomes:
$$\text{TotalRegret}(T) \leq \text{BayesRegret}(T) + \frac{T H \epsilon_{\text{comp}}}{1 - \gamma}$$
where $\gamma$ is the discount factor.
\end{theorem>

This quantifies the tradeoff between computational efficiency and statistical performance in real-time applications.